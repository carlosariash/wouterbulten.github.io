---
layout: post
title:  "First steps with GPy"
date:   2015-02-13 14:44
categories: blog slac gp
tags: featured
image: /assets/article_images/introgp_regress_preoptim.png
math: true
---

A random process, a collection of random variables, is said to be a _Gaussian process (GP)_[^1] if any finite number of these variables have a joint Gaussian distribution; i.e. the relation between variables follows a Gaussian distribution, this says something about the smoothness of functions generated by these processes. Guassian processes are used for many tasks in machine learning; from classification to regression and latent variable models.

A lot of work on this subject is done by the machine learning group at the [University of Sheffield](http://ml.dcs.shef.ac.uk/) which maintain and develop the _[GPy](http://github.com/SheffieldML/GPy)_ package: a framework, written in python, for GP's. In this post we will take a first step in using this framework.

## Installing GPy

GPy can be installed using _pip_ which is probably the most convenient way. Just run

	pip install gpy

and GPy and it's dependencies should be installed. 

> _Note:_ GPy does not work smoothly with Python 3+. An [attempt](https://github.com/wouterbulten/GPy/tree/python3) to do a 'quick and dirty' conversion did not work reliably. I suggest you use an 2.* installation of Python in combination with GPy until its authors update the framework.

If you install _nose_ you can run the tests embedded in GPy to see if the installation was successful:

	pip install nose
	ipython
	
{% highlight python %}
import GPy
GPy.tests()
{% endhighlight %}

## Simple regression

We'll test the GPy framework by solving a simple regression problem. Consider the following formula:

$$
y = \frac{1}{4} x^2 + \epsilon
$$

with $\epsilon \in \mathcal{N}(0,.25)$. We can then generate our train on the interval [-5,5] set with:

{% highlight python %}
X = np.random.uniform(-5, 5, (20,1))
Y = np.array([0.25 * (a*a) + (.25 * np.random.randn()) for a in X])
{% endhighlight %}

Our regression model requires a kernel for which we choose a RBF kernel with a default variance and lengthscale of 1. With this kernel we are ready to optimize the model:

{% highlight python %}
kernel = GPy.kern.RBF(input_dim=1)
m = GPy.models.GPRegression(X,Y,kernel)
print m
m.plot()
{% endhighlight %}

![Plot of the GP model before optimization. Black x's are training points. The line is the predicted function. The shaded region corresponds to the roughly 95% confidence interval.](/assets/inline_images/introgp_regress_preoptim.png)

{% highlight python %}
m.optimize()
print m
m.plot()
{% endhighlight %}

	Name                 : GP regression
	Log-likelihood       : -13.3897302116
	Number of Parameters : 3
	Parameters:
	  GP_regression.           |       Value       |  Constraint  |  Prior  |  Tied to
	  rbf.variance             |    465.887938367  |     +ve      |         |         
	  rbf.lengthscale          |    10.2848156804  |     +ve      |         |         
	  Gaussian_noise.variance  |  0.0488157865927  |     +ve      |         |         

![Plot of the GP model after optimization.](/assets/inline_images/introgp_regress_optim.png)

After the parameters of the model have been optimized the GP has a significant better fit on the underlying function. We can see that all parameter values have changed from their default values.

## Alternatives

There are also other GP frameworks for python apart from _GPy_. The _[pyGPs](https://github.com/marionmari/pyGPs)_ ([docs](http://www-ai.cs.uni-dortmund.de/weblab/static/api_docs/pyGPs/)) package is a good example with support for both classification and regression. 

## References 
[^1]: We follow here the definitions as described by _Rasmussen, C. E., & Williams, C. (2006). Gaussian Processes for Machine Learning. the MIT Press._